{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an xG Model from Scratch\n",
    "### Columbus Crew Dashboard — Portfolio Project\n",
    "\n",
    "---\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Explains the math behind Expected Goals (xG)\n",
    "2. Loads real shot data from StatsBomb open data\n",
    "3. Engineers features (distance, angle, technique)\n",
    "4. Trains a logistic regression model\n",
    "5. Evaluates it with proper metrics\n",
    "6. Exports the model for use in the dashboard\n",
    "\n",
    "> **Why logistic regression?** It's the industry standard starting point for xG modeling. It's interpretable (you can explain *why* each shot has its probability), fast to train, and legitimate — StatsBomb themselves use logistic regression as a baseline."
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Math (Read This Before Running Any Code)\n",
    "\n",
    "### What is a probability model?\n",
    "\n",
    "We want to answer: **given a shot, what's the probability it becomes a goal?**\n",
    "\n",
    "We can't use regular linear regression for this because probabilities must stay between 0 and 1 — linear regression has no such constraint. That's where **logistic regression** comes in.\n",
    "\n",
    "### The Sigmoid Function\n",
    "\n",
    "Logistic regression applies a **sigmoid function** to a linear equation:\n",
    "\n",
    "$$P(\\text{goal}) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...)}}$$\n",
    "\n",
    "Breaking this down:\n",
    "- $x_1, x_2, ...$ are your **features** (distance, angle, technique)\n",
    "- $\\beta_0, \\beta_1, \\beta_2, ...$ are **coefficients** the model learns from data\n",
    "- The sigmoid function squishes any number into a value between 0 and 1\n",
    "- The output is a **probability** — e.g. 0.73 means 73% chance of a goal\n",
    "\n",
    "### How does the model \"learn\"?\n",
    "\n",
    "Training means finding the $\\beta$ values that make the model's predictions match reality as closely as possible. It does this by minimizing **log loss** (also called binary cross-entropy):\n",
    "\n",
    "$$\\text{Log Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{p}_i) + (1 - y_i) \\log(1 - \\hat{p}_i) \\right]$$\n",
    "\n",
    "Where:\n",
    "- $y_i = 1$ if the shot was a goal, $0$ if not\n",
    "- $\\hat{p}_i$ is the model's predicted probability\n",
    "- The model adjusts $\\beta$ values to minimize this loss\n",
    "\n",
    "**Intuition:** If the model says a shot has a 90% chance of being a goal and it actually IS a goal, log loss is low (good). If the model says 90% chance and it's NOT a goal, log loss is high (bad). The model learns to be well-calibrated.\n",
    "\n",
    "### Key features for xG\n",
    "\n",
    "| Feature | Why it matters |\n",
    "|---|---|\n",
    "| Distance to goal | Closer shots score more often |\n",
    "| Shot angle | Wider angles are harder |\n",
    "| Technique (header vs foot) | Headers convert at ~60% the rate of foot shots |\n",
    "| Under pressure | Pressured shots convert less |\n",
    "| From open play vs set piece | Set piece shots are often lower quality |\n",
    "\n",
    "---"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Setup & Data Loading"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# Run this cell once, then comment it out\n",
    "# !pip install statsbombpy scikit-learn pandas numpy matplotlib seaborn mplsoccer joblib"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "from statsbombpy import sb\n",
    "from mplsoccer import Pitch\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, log_loss, brier_score_loss,\n",
    "    classification_report, RocCurveDisplay, CalibrationDisplay\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "\n",
    "# Crew colors\n",
    "CREW_GOLD  = '#F5C518'\n",
    "CREW_BLACK = '#14130F'\n",
    "\n",
    "print('All imports successful!')"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load StatsBomb open data ──────────────────────────────\n",
    "# StatsBomb provides free event data for select competitions.\n",
    "# We'll use multiple competitions to maximize our training data.\n",
    "# More data = better model.\n",
    "\n",
    "print('Available competitions:')\n",
    "comps = sb.competitions()\n",
    "print(comps[['competition_name', 'season_name', 'competition_id', 'season_id']].to_string())"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll pull shots from multiple competitions for a robust training set.\n",
    "# These are all freely available from StatsBomb.\n",
    "COMPETITIONS = [\n",
    "    {'competition_id': 2,   'season_id': 27},   # Premier League 2015/16\n",
    "    {'competition_id': 11,  'season_id': 1},    # La Liga 2005/06\n",
    "    {'competition_id': 49,  'season_id': 3},    # Women's World Cup 2019\n",
    "    {'competition_id': 72,  'season_id': 30},   # NWSL 2018\n",
    "]\n",
    "\n",
    "all_shots = []\n",
    "\n",
    "for comp in COMPETITIONS:\n",
    "    cid, sid = comp['competition_id'], comp['season_id']\n",
    "    try:\n",
    "        matches = sb.matches(competition_id=cid, season_id=sid)\n",
    "        print(f'Competition {cid}/{sid}: {len(matches)} matches')\n",
    "\n",
    "        for match_id in matches['match_id']:\n",
    "            events = sb.events(match_id=match_id)\n",
    "            shots  = events[events['type'] == 'Shot'].copy()\n",
    "            if len(shots) > 0:\n",
    "                shots['competition_id'] = cid\n",
    "                shots['season_id']      = sid\n",
    "                all_shots.append(shots)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'  Skipping {cid}/{sid}: {e}')\n",
    "\n",
    "shots_raw = pd.concat(all_shots, ignore_index=True)\n",
    "print(f'\\nTotal shots loaded: {len(shots_raw):,}')\n",
    "print(f'Goal rate: {(shots_raw[\"shot_outcome\"] == \"Goal\").mean():.1%}')"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Feature Engineering\n",
    "\n",
    "This is where domain knowledge meets data science. We need to transform raw shot coordinates into meaningful predictors.\n",
    "\n",
    "**StatsBomb pitch coordinates:**\n",
    "- The pitch is 120 x 80 units\n",
    "- Goal center is at (120, 40)\n",
    "- (0, 0) is the bottom-left corner"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Transform raw StatsBomb shot data into model features.\n",
    "    Returns a clean DataFrame ready for training.\n",
    "    \"\"\"\n",
    "    shots = df.copy()\n",
    "\n",
    "    # ── Extract shot coordinates ──────────────────────────\n",
    "    shots['x'] = shots['location'].apply(lambda loc: loc[0] if isinstance(loc, list) else np.nan)\n",
    "    shots['y'] = shots['location'].apply(lambda loc: loc[1] if isinstance(loc, list) else np.nan)\n",
    "\n",
    "    # ── Goal center (StatsBomb coordinates) ──────────────\n",
    "    GOAL_X = 120\n",
    "    GOAL_Y = 40\n",
    "\n",
    "    # Feature 1: Distance to goal center\n",
    "    # Euclidean distance — closer shots are harder to save\n",
    "    shots['distance'] = np.sqrt(\n",
    "        (shots['x'] - GOAL_X) ** 2 +\n",
    "        (shots['y'] - GOAL_Y) ** 2\n",
    "    )\n",
    "\n",
    "    # Feature 2: Shot angle (in radians, then converted to degrees)\n",
    "    # The angle between the shot location and the two goalposts\n",
    "    # A wider angle = better chance = higher xG\n",
    "    GOAL_POST_Y1 = 36  # left post\n",
    "    GOAL_POST_Y2 = 44  # right post\n",
    "\n",
    "    # Vector from shot location to each post\n",
    "    a = np.sqrt((shots['x'] - GOAL_X)**2 + (shots['y'] - GOAL_POST_Y1)**2)\n",
    "    b = np.sqrt((shots['x'] - GOAL_X)**2 + (shots['y'] - GOAL_POST_Y2)**2)\n",
    "    c = np.abs(GOAL_POST_Y2 - GOAL_POST_Y1)  # width of goal = 8 units\n",
    "\n",
    "    # Law of cosines to find the angle\n",
    "    cos_angle = (a**2 + b**2 - c**2) / (2 * a * b)\n",
    "    cos_angle = np.clip(cos_angle, -1, 1)  # prevent numerical errors\n",
    "    shots['angle_deg'] = np.degrees(np.arccos(cos_angle))\n",
    "\n",
    "    # Feature 3: Is header?\n",
    "    shots['is_header'] = (shots['shot_body_part'] == 'Head').astype(int)\n",
    "\n",
    "    # Feature 4: Under pressure?\n",
    "    shots['under_pressure'] = shots['under_pressure'].fillna(False).astype(int)\n",
    "\n",
    "    # Feature 5: Play pattern (open play = 1, set piece = 0)\n",
    "    open_play_patterns = ['Regular Play', 'From Goal Kick', 'From Keeper']\n",
    "    shots['is_open_play'] = shots['play_pattern'].isin(open_play_patterns).astype(int)\n",
    "\n",
    "    # Feature 6: Distance squared (non-linear relationship — drop-off is steep)\n",
    "    shots['distance_sq'] = shots['distance'] ** 2\n",
    "\n",
    "    # ── Target variable ───────────────────────────────────\n",
    "    shots['is_goal'] = (shots['shot_outcome'] == 'Goal').astype(int)\n",
    "\n",
    "    # ── Remove own goals (not predictable the same way) ───\n",
    "    if 'shot_type' in shots.columns:\n",
    "        shots = shots[shots['shot_type'] != 'Own Goal']\n",
    "\n",
    "    # ── Drop rows with missing key features ───────────────\n",
    "    features = ['distance', 'angle_deg', 'is_header', 'under_pressure',\n",
    "                'is_open_play', 'distance_sq', 'is_goal']\n",
    "    shots = shots.dropna(subset=['x', 'y', 'distance', 'angle_deg'])\n",
    "\n",
    "    return shots[features + ['x', 'y', 'shot_outcome', 'shot_body_part', 'play_pattern']]\n",
    "\n",
    "\n",
    "shots_featured = engineer_features(shots_raw)\n",
    "\n",
    "print(f'Shots after feature engineering: {len(shots_featured):,}')\n",
    "print(f'Goal rate: {shots_featured[\"is_goal\"].mean():.1%}')\n",
    "print()\n",
    "shots_featured.describe()"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Visualize key feature distributions ──────────────────\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "fig.patch.set_facecolor('#F9F6F0')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_facecolor('#F9F6F0')\n",
    "\n",
    "# Distance distribution by outcome\n",
    "goals     = shots_featured[shots_featured['is_goal'] == 1]\n",
    "non_goals = shots_featured[shots_featured['is_goal'] == 0]\n",
    "\n",
    "axes[0].hist(non_goals['distance'], bins=40, alpha=0.6, color='#888', label='No Goal', density=True)\n",
    "axes[0].hist(goals['distance'],     bins=40, alpha=0.8, color=CREW_GOLD, label='Goal',    density=True)\n",
    "axes[0].set_title('Distance Distribution', fontweight='bold')\n",
    "axes[0].set_xlabel('Distance (units)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Angle distribution by outcome\n",
    "axes[1].hist(non_goals['angle_deg'], bins=40, alpha=0.6, color='#888', label='No Goal', density=True)\n",
    "axes[1].hist(goals['angle_deg'],     bins=40, alpha=0.8, color=CREW_GOLD, label='Goal',    density=True)\n",
    "axes[1].set_title('Shot Angle Distribution', fontweight='bold')\n",
    "axes[1].set_xlabel('Angle (degrees)')\n",
    "axes[1].legend()\n",
    "\n",
    "# Goal rate by body part\n",
    "body_part_rates = shots_featured.groupby('shot_body_part')['is_goal'].mean().sort_values(ascending=False)\n",
    "bars = axes[2].bar(body_part_rates.index, body_part_rates.values, color=[CREW_GOLD, '#888', '#555', '#333'])\n",
    "axes[2].set_title('Goal Rate by Body Part', fontweight='bold')\n",
    "axes[2].set_ylabel('Goal Rate')\n",
    "axes[2].yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\n",
    "\n",
    "plt.suptitle('Feature Distributions — Shot Data', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('assets/feature_distributions.png', dpi=150, bbox_inches='tight', facecolor='#F9F6F0')\n",
    "plt.show()\n",
    "print('Insight: Goals cluster at shorter distances and wider angles — exactly what we expect.')"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Train the Model\n",
    "\n",
    "### Train/Test Split — Why It Matters\n",
    "\n",
    "We split data into **training** (80%) and **testing** (20%) sets.\n",
    "\n",
    "- **Training set**: The model sees these shots and learns from them\n",
    "- **Test set**: The model has never seen these — we use them to measure real performance\n",
    "\n",
    "Without this split, you'd be measuring how well the model memorized your data, not how well it generalizes. A model that only works on data it's already seen is useless in practice.\n",
    "\n",
    "### Feature Scaling\n",
    "\n",
    "Logistic regression is sensitive to feature scale. Distance ranges from 0–60, angle from 0–90. We use `StandardScaler` to normalize them to a common scale (mean=0, std=1) so no single feature dominates just because its numbers are bigger."
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['distance', 'angle_deg', 'is_header', 'under_pressure', 'is_open_play', 'distance_sq']\n",
    "TARGET   = 'is_goal'\n",
    "\n",
    "X = shots_featured[FEATURES]\n",
    "y = shots_featured[TARGET]\n",
    "\n",
    "# 80/20 split, stratified so goal rate is preserved in both halves\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,       # makes results reproducible\n",
    "    stratify=y             # preserves ~10% goal rate in both splits\n",
    ")\n",
    "\n",
    "print(f'Training shots : {len(X_train):,}  (Goal rate: {y_train.mean():.1%})')\n",
    "print(f'Test shots     : {len(X_test):,}   (Goal rate: {y_test.mean():.1%})')"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Build pipeline: scale then fit ───────────────────────\n",
    "# A Pipeline chains steps so scaling and fitting happen together.\n",
    "# This prevents data leakage (scaler learns ONLY from training data).\n",
    "\n",
    "xg_model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(\n",
    "        C=1.0,              # regularization strength (1.0 = default, balanced)\n",
    "        max_iter=1000,      # iterations to converge\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "xg_model.fit(X_train, y_train)\n",
    "\n",
    "print('Model trained!')\n",
    "\n",
    "# ── Inspect coefficients ──────────────────────────────────\n",
    "# This is the interpretability advantage of logistic regression.\n",
    "# Negative = reduces goal probability, Positive = increases it.\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature':     FEATURES,\n",
    "    'Coefficient': xg_model.named_steps['logreg'].coef_[0]\n",
    "}).sort_values('Coefficient')\n",
    "\n",
    "print('\\nModel Coefficients (scaled):')\n",
    "print(coef_df.to_string(index=False))\n",
    "print('\\nInterpretation:')\n",
    "print('  Negative = REDUCES goal probability (e.g. distance hurts)')\n",
    "print('  Positive = INCREASES goal probability (e.g. wider angle helps)')"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coefficients\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "fig.patch.set_facecolor('#F9F6F0')\n",
    "ax.set_facecolor('#F9F6F0')\n",
    "\n",
    "colors = [CREW_GOLD if c > 0 else '#555' for c in coef_df['Coefficient']]\n",
    "bars   = ax.barh(coef_df['Feature'], coef_df['Coefficient'], color=colors, edgecolor='none')\n",
    "ax.axvline(0, color='#333', linewidth=0.8)\n",
    "ax.set_title('Logistic Regression Coefficients\\n(Gold = increases xG, Grey = decreases xG)',\n",
    "             fontweight='bold')\n",
    "ax.set_xlabel('Coefficient (scaled)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('assets/coefficients.png', dpi=150, bbox_inches='tight', facecolor='#F9F6F0')\n",
    "plt.show()"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Evaluate the Model\n",
    "\n",
    "Three metrics matter for an xG model:\n",
    "\n",
    "| Metric | What it measures | Good value |\n",
    "|---|---|---|\n",
    "| **ROC-AUC** | How well the model ranks shots by quality | > 0.75 |\n",
    "| **Log Loss** | How well-calibrated the probabilities are | < 0.30 |\n",
    "| **Brier Score** | Mean squared error of probabilities | < 0.09 |\n",
    "\n",
    "**Why not accuracy?** Because ~90% of shots don't result in goals. A model that always predicts 'no goal' would be 90% accurate — but completely useless. AUC and log loss tell you whether the *probabilities* are meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Generate predictions ──────────────────────────────────\n",
    "y_proba = xg_model.predict_proba(X_test)[:, 1]  # probability of goal\n",
    "\n",
    "auc    = roc_auc_score(y_test, y_proba)\n",
    "logloss = log_loss(y_test, y_proba)\n",
    "brier  = brier_score_loss(y_test, y_proba)\n",
    "\n",
    "print('=' * 40)\n",
    "print('  MODEL EVALUATION (Test Set)')\n",
    "print('=' * 40)\n",
    "print(f'  ROC-AUC    : {auc:.4f}   (target > 0.75)')\n",
    "print(f'  Log Loss   : {logloss:.4f}   (target < 0.30)')\n",
    "print(f'  Brier Score: {brier:.4f}   (target < 0.09)')\n",
    "print('=' * 40)\n",
    "\n",
    "if auc > 0.75:\n",
    "    print('\\n  AUC looks good! The model ranks shot quality well.')\n",
    "else:\n",
    "    print('\\n  AUC is low — consider adding more features or training data.')"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── ROC Curve & Calibration Plot ──────────────────────────\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n",
    "fig.patch.set_facecolor('#F9F6F0')\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.set_facecolor('#F9F6F0')\n",
    "\n",
    "# ROC Curve\n",
    "# The closer to the top-left corner, the better the model discriminates\n",
    "RocCurveDisplay.from_predictions(\n",
    "    y_test, y_proba,\n",
    "    name=f'xG Model (AUC={auc:.3f})',\n",
    "    color=CREW_GOLD,\n",
    "    ax=ax1\n",
    ")\n",
    "ax1.plot([0,1], [0,1], 'k--', alpha=0.4, label='Random classifier')\n",
    "ax1.set_title('ROC Curve', fontweight='bold')\n",
    "ax1.legend()\n",
    "\n",
    "# Calibration Plot\n",
    "# A well-calibrated model: when it says 20% xG, shots score ~20% of the time\n",
    "CalibrationDisplay.from_predictions(\n",
    "    y_test, y_proba,\n",
    "    n_bins=10,\n",
    "    name='xG Model',\n",
    "    color=CREW_GOLD,\n",
    "    ax=ax2\n",
    ")\n",
    "ax2.set_title('Calibration Curve\\n(Diagonal = perfectly calibrated)', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Model Evaluation', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('assets/model_evaluation.png', dpi=150, bbox_inches='tight', facecolor='#F9F6F0')\n",
    "plt.show()"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Apply the Model — Generate xG for Sample Shots\n",
    "\n",
    "This section demonstrates how to use your trained model to predict xG for any shot, given its coordinates and context. This is what you'd run on Columbus Crew shot data."
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_xg(distance, angle_deg, is_header=0, under_pressure=0, is_open_play=1):\n",
    "    \"\"\"\n",
    "    Predict xG for a single shot.\n",
    "    Returns a probability between 0 and 1.\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame([{\n",
    "        'distance':      distance,\n",
    "        'angle_deg':     angle_deg,\n",
    "        'is_header':     is_header,\n",
    "        'under_pressure': under_pressure,\n",
    "        'is_open_play':  is_open_play,\n",
    "        'distance_sq':   distance ** 2\n",
    "    }])\n",
    "    return xg_model.predict_proba(features)[0][1]\n",
    "\n",
    "\n",
    "# ── Test with realistic scenarios ─────────────────────────\n",
    "scenarios = [\n",
    "    ('Penalty kick',              11,  37, 0, 0, 0),\n",
    "    ('Close range header',         8,  45, 1, 0, 1),\n",
    "    ('One-on-one, open play',     14,  40, 0, 0, 1),\n",
    "    ('Edge of box, under pressure',22,  25, 0, 1, 1),\n",
    "    ('Long-range effort',          35,  15, 0, 0, 1),\n",
    "    ('Half-volley from distance',  40,  10, 0, 0, 1),\n",
    "]\n",
    "\n",
    "print(f'{\"Scenario\":<35} {\"xG\":>6}')\n",
    "print('-' * 43)\n",
    "for name, dist, angle, head, press, open_play in scenarios:\n",
    "    xg = predict_xg(dist, angle, head, press, open_play)\n",
    "    bar = '█' * int(xg * 30)\n",
    "    print(f'{name:<35} {xg:>5.3f}  {bar}')"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── xG Probability Heatmap ────────────────────────────────\n",
    "# Visualize xG across the pitch for open-play foot shots\n",
    "# This is a great visual for your README\n",
    "\n",
    "GOAL_X, GOAL_Y = 120, 40\n",
    "\n",
    "# Grid of shot locations in the attacking half\n",
    "xs = np.linspace(60, 119, 120)\n",
    "ys = np.linspace(0, 80, 80)\n",
    "xx, yy = np.meshgrid(xs, ys)\n",
    "\n",
    "# Calculate distance and angle for each point\n",
    "dist_grid  = np.sqrt((xx - GOAL_X)**2 + (yy - GOAL_Y)**2)\n",
    "\n",
    "GOAL_POST_Y1, GOAL_POST_Y2 = 36, 44\n",
    "a = np.sqrt((xx - GOAL_X)**2 + (yy - GOAL_POST_Y1)**2)\n",
    "b = np.sqrt((xx - GOAL_X)**2 + (yy - GOAL_POST_Y2)**2)\n",
    "c = abs(GOAL_POST_Y2 - GOAL_POST_Y1)\n",
    "cos_angle   = np.clip((a**2 + b**2 - c**2) / (2 * a * b), -1, 1)\n",
    "angle_grid  = np.degrees(np.arccos(cos_angle))\n",
    "\n",
    "# Predict xG for each grid point\n",
    "grid_features = pd.DataFrame({\n",
    "    'distance':       dist_grid.ravel(),\n",
    "    'angle_deg':      angle_grid.ravel(),\n",
    "    'is_header':      np.zeros(dist_grid.size),\n",
    "    'under_pressure': np.zeros(dist_grid.size),\n",
    "    'is_open_play':   np.ones(dist_grid.size),\n",
    "    'distance_sq':    dist_grid.ravel() ** 2\n",
    "})\n",
    "\n",
    "xg_grid = xg_model.predict_proba(grid_features)[:, 1].reshape(xx.shape)\n",
    "\n",
    "# Plot on a pitch\n",
    "pitch = Pitch(pitch_type='statsbomb', pitch_color=CREW_BLACK, line_color='#555')\n",
    "fig, ax = pitch.draw(figsize=(13, 8))\n",
    "fig.set_facecolor(CREW_BLACK)\n",
    "\n",
    "# Only show attacking half\n",
    "heatmap = ax.contourf(\n",
    "    xx, yy, xg_grid,\n",
    "    levels=20,\n",
    "    cmap='YlOrRd',\n",
    "    alpha=0.85,\n",
    "    vmin=0, vmax=0.5\n",
    ")\n",
    "\n",
    "cbar = plt.colorbar(heatmap, ax=ax, fraction=0.03, pad=0.02)\n",
    "cbar.set_label('xG Probability', color='white')\n",
    "cbar.ax.yaxis.set_tick_params(color='white')\n",
    "plt.setp(cbar.ax.yaxis.get_ticklabels(), color='white')\n",
    "\n",
    "ax.set_title('xG Probability Map — Open Play Foot Shot\\n(Brighter = Higher Chance of Scoring)',\n",
    "             color='white', fontsize=13, fontweight='bold', pad=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs('assets', exist_ok=True)\n",
    "plt.savefig('assets/xg_heatmap.png', dpi=150, bbox_inches='tight', facecolor=CREW_BLACK)\n",
    "plt.show()\n",
    "print('Saved to assets/xg_heatmap.png — this is your hero image for the README.')"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Export the Model"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Save the trained model ────────────────────────────────\n",
    "os.makedirs('models', exist_ok=True)\n",
    "joblib.dump(xg_model, 'models/xg_model.pkl')\n",
    "print('Model saved to models/xg_model.pkl')\n",
    "\n",
    "# ── Save feature list ─────────────────────────────────────\n",
    "model_meta = {\n",
    "    'features':    FEATURES,\n",
    "    'target':      TARGET,\n",
    "    'auc':         round(auc, 4),\n",
    "    'log_loss':    round(logloss, 4),\n",
    "    'brier_score': round(brier, 4),\n",
    "    'training_shots': len(X_train),\n",
    "    'notes': 'Logistic regression trained on StatsBomb open data. Apply to any shot with distance, angle, technique, pressure, and play pattern.'\n",
    "}\n",
    "\n",
    "with open('models/model_meta.json', 'w') as f:\n",
    "    json.dump(model_meta, f, indent=2)\n",
    "\n",
    "print('Model metadata saved to models/model_meta.json')\n",
    "print()\n",
    "print('To load and use this model later:')\n",
    "print('  import joblib')\n",
    "print('  model = joblib.load(\"models/xg_model.pkl\")')\n",
    "print('  xg = model.predict_proba(features_df)[:, 1]')"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: What You Built\n",
    "\n",
    "| Step | What happened |\n",
    "|---|---|\n",
    "| Data | Loaded real shot events from StatsBomb open data |\n",
    "| Features | Calculated distance, angle, technique, pressure from raw coordinates |\n",
    "| Model | Trained a logistic regression using scikit-learn Pipeline |\n",
    "| Math | Sigmoid function + log loss minimization |\n",
    "| Evaluation | ROC-AUC, log loss, Brier score, calibration curve |\n",
    "| Output | Saved model + heatmap + coefficient chart |\n",
    "\n",
    "**Next notebook:** Load this model, apply it to Columbus Crew shot location data, and build the match-by-match xG dashboard.\n",
    "\n",
    "**Interview talking point:** *\"I built a logistic regression xG model trained on StatsBomb open data. I engineered distance, angle, body part, pressure, and play pattern as features. The model outputs a calibrated probability between 0 and 1 — when it says a shot has 0.20 xG, shots with those characteristics score roughly 20% of the time. I evaluated it with ROC-AUC and a calibration curve rather than accuracy, because accuracy is misleading when only 10% of shots are goals.\"*"
   ]
  }

 ]
}
